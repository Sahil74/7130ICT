{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahil74/7130ICT/blob/main/LeNet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDbJWoO1yO8e"
      },
      "source": [
        "# Image Classification with CNN - LeNet5 architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzQxqD6HyO8i"
      },
      "source": [
        "In this exercise, we will apply the LeNet5 algorithm to the Fashion MNIST dataset and improve your performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFyVotRvyO8j"
      },
      "source": [
        "We will first download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTHLyL1fyO8j",
        "outputId": "6a6f92a1-029e-48b2-ef33-03cdf0089b62",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# TODO: Load the dataset\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# # # If your computer is slow, try to use a subset of data, e.g.\n",
        "# X_train = X_train[:10000]\n",
        "# y_train = y_train[:10000]\n",
        "# X_test = X_test[:2000]\n",
        "# y_test = y_test[:2000]\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8ShXIANyO8l"
      },
      "source": [
        "As you already know, this dataset contains 10 classes:\n",
        "* 0:\tT-shirt/top\n",
        "* 1:\tTrouser\n",
        "* 2:\tPullover\n",
        "* 3:\tDress\n",
        "* 4:\tCoat\n",
        "* 5:\tSandal\n",
        "* 6:\tShirt\n",
        "* 7:\tSneaker\n",
        "* 8:\tBag\n",
        "* 9:\tAnkle boot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BvNG0PbyO8l"
      },
      "source": [
        "You can have a look at some images if needed, even if you already know them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "lnjqgv-GyO8m",
        "outputId": "2d8f1f46-614f-424a-8bed-4364c2910b79",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: Explore the data, display some input images\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "label_class = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "\n",
        "idx = np.random.randint(X_train.shape[0])\n",
        "plt.imshow(X_train[idx],cmap=\"gray_r\")\n",
        "plt.title(label_class[y_train[idx]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdYH6XW1yO8n"
      },
      "source": [
        "Make the data preparation and preprocessing: scale and reshape the data, put the labels to the good shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fjv8XMPByO8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0832eb-1179-4439-8358-ffba46eb85c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# TODO: Make the data preparation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train,num_classes=10)\n",
        "y_test_cat = to_categorical(y_test,num_classes=10)\n",
        "\n",
        "X_train_norm = X_train/255\n",
        "X_test_norm = X_test/255\n",
        "\n",
        "X_train_norm = X_train_norm.reshape(X_train_norm.shape[0],28,28,1)\n",
        "X_test_norm = X_test_norm.reshape(X_test_norm.shape[0],28,28,1)\n",
        "\n",
        "X_train_norm.shape #Should be (60000, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9LKzxR9yO8o"
      },
      "source": [
        "Now build the LeNet5 architecture. You can reuse the one of the course, or try to build it by yourself.\n",
        "\n",
        "The architecture is the following:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WteTU2FPIVMkBKmMxGpFm5OjsX-szTbB\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GKyMFlL6yO8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e0fedb-7c61-4b50-d2fc-4094781f874f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " C1 (Conv2D)                 (None, 26, 26, 6)         60        \n",
            "                                                                 \n",
            " S2 (MaxPooling2D)           (None, 13, 13, 6)         0         \n",
            "                                                                 \n",
            " C3 (Conv2D)                 (None, 11, 11, 16)        880       \n",
            "                                                                 \n",
            " S4 (MaxPooling2D)           (None, 5, 5, 16)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " C5 (Dense)                  (None, 120)               48120     \n",
            "                                                                 \n",
            " F5 (Dense)                  (None, 84)                10164     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60,074\n",
            "Trainable params: 60,074\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# TODO: Build your model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import MaxPooling2D, Conv2D, Flatten, Dense\n",
        "\n",
        "\n",
        "def lenet5():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    # Layer C1\n",
        "    model.add(Conv2D(filters=6, name='C1', kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
        "    # Layer S2\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), name='S2'))\n",
        "    # Layer C3\n",
        "    model.add(Conv2D(filters=16, name='C3', kernel_size=(3, 3), activation='relu'))\n",
        "    # Layer S4\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), name='S4'))\n",
        "    # Before going into layer C5, we flatten our units\n",
        "    model.add(Flatten())\n",
        "    # Layer C5\n",
        "    model.add(Dense(120,activation= 'relu', name='C5'))\n",
        "    # Layer F6\n",
        "    model.add(Dense(84,activation= 'relu', name='F5'))\n",
        "    # Output layer\n",
        "    model.add(Dense(units=10, activation = 'softmax'))\n",
        "    \n",
        "    return model\n",
        "lenet5().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1qBEauqyO8p"
      },
      "source": [
        "Now compile and fit your model on your training data. Since this is a multiclass classification, the loss is not `binary_crossentropy` anymore, but `categorical_crossentropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPL3aKnyyO8p",
        "outputId": "61a121e6-7bf5-45dc-d394-2fac2b875924",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "30/30 [==============================] - 14s 31ms/step - loss: 1.7123 - accuracy: 0.5295 - val_loss: 0.9543 - val_accuracy: 0.6606\n",
            "Epoch 2/20\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.7852 - accuracy: 0.7138 - val_loss: 0.7074 - val_accuracy: 0.7355\n",
            "Epoch 3/20\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.6425 - accuracy: 0.7553 - val_loss: 0.6162 - val_accuracy: 0.7673\n",
            "Epoch 4/20\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5821 - accuracy: 0.7786 - val_loss: 0.5760 - val_accuracy: 0.7803\n",
            "Epoch 5/20\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.5432 - accuracy: 0.7956 - val_loss: 0.5645 - val_accuracy: 0.7884\n",
            "Epoch 6/20\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 0.5209 - accuracy: 0.8064 - val_loss: 0.5244 - val_accuracy: 0.8044\n",
            "Epoch 7/20\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 0.4921 - accuracy: 0.8202 - val_loss: 0.5042 - val_accuracy: 0.8163\n",
            "Epoch 8/20\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.4706 - accuracy: 0.8301 - val_loss: 0.4858 - val_accuracy: 0.8231\n",
            "Epoch 9/20\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.4514 - accuracy: 0.8387 - val_loss: 0.4633 - val_accuracy: 0.8330\n",
            "Epoch 10/20\n",
            "30/30 [==============================] - 1s 17ms/step - loss: 0.4376 - accuracy: 0.8460 - val_loss: 0.4482 - val_accuracy: 0.8425\n",
            "Epoch 11/20\n",
            "30/30 [==============================] - 1s 26ms/step - loss: 0.4270 - accuracy: 0.8494 - val_loss: 0.4384 - val_accuracy: 0.8421\n",
            "Epoch 12/20\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.4100 - accuracy: 0.8548 - val_loss: 0.4248 - val_accuracy: 0.8484\n",
            "Epoch 13/20\n",
            "30/30 [==============================] - 1s 36ms/step - loss: 0.4009 - accuracy: 0.8582 - val_loss: 0.4154 - val_accuracy: 0.8522\n",
            "Epoch 14/20\n",
            "30/30 [==============================] - 1s 25ms/step - loss: 0.3953 - accuracy: 0.8590 - val_loss: 0.4152 - val_accuracy: 0.8513\n",
            "Epoch 15/20\n",
            "30/30 [==============================] - 1s 29ms/step - loss: 0.3875 - accuracy: 0.8630 - val_loss: 0.4084 - val_accuracy: 0.8536\n",
            "Epoch 16/20\n",
            "30/30 [==============================] - 0s 16ms/step - loss: 0.3772 - accuracy: 0.8666 - val_loss: 0.4031 - val_accuracy: 0.8539\n",
            "Epoch 17/20\n",
            "30/30 [==============================] - 0s 15ms/step - loss: 0.3783 - accuracy: 0.8645 - val_loss: 0.3945 - val_accuracy: 0.8583\n",
            "Epoch 18/20\n",
            "30/30 [==============================] - 0s 14ms/step - loss: 0.3697 - accuracy: 0.8687 - val_loss: 0.3870 - val_accuracy: 0.8638\n",
            "Epoch 19/20\n",
            "30/30 [==============================] - 1s 21ms/step - loss: 0.3661 - accuracy: 0.8697 - val_loss: 0.3977 - val_accuracy: 0.8545\n",
            "Epoch 20/20\n",
            "30/30 [==============================] - 1s 23ms/step - loss: 0.3556 - accuracy: 0.8734 - val_loss: 0.3737 - val_accuracy: 0.8652\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f15fb986c20>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# TODO: Compile and fit your model\n",
        "import os\n",
        "\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True' #https://stackoverflow.com/questions/53014306/error-15-initializing-libiomp5-dylib-but-found-libiomp5-dylib-already-initial\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "model = lenet5()\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define now our callbacks\n",
        "# callbacks = [EarlyStopping(monitor='val_loss', patience=10), TensorBoard(log_dir='./keras-logs', histogram_freq=0, write_graph=True, write_images=True)]\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=10)]\n",
        "\n",
        "# Finally fit the model\n",
        "model.fit(x=X_train_norm, y=y_train_cat, validation_data=(X_test_norm, y_test_cat), epochs=20, batch_size=2048, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf-SqjjOyO8q"
      },
      "source": [
        "Have a look at the tensorboard and see if it gives a deeper understanding of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2FTj7TSyO8q"
      },
      "source": [
        "Compute then the accuracy of your model. Is it better than a regular MLP used before?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPjJoMQZyO8q",
        "outputId": "3a1ecadc-1dfb-4967-d4a0-fce181f62a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 0s 3ms/step\n",
            "10/10 [==============================] - 0s 13ms/step\n",
            "accuracy on train with NN: 0.8756666666666667\n",
            "accuracy on test with NN: 0.8652\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute the accuracy of your model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "batch_size = 1024\n",
        "y_pred_train = to_categorical(model.predict(X_train_norm,batch_size=batch_size).argmax(axis=1), num_classes=10)\n",
        "y_pred_test = to_categorical(model.predict(X_test_norm,batch_size=batch_size).argmax(axis=1), num_classes=10)\n",
        "\n",
        "print('accuracy on train with NN:', accuracy_score(y_pred_train, y_train_cat))\n",
        "print('accuracy on test with NN:', accuracy_score(y_pred_test, y_test_cat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vulsgHiyO8q"
      },
      "source": [
        "We will now add image augmentation to improve our results, especially we will try to reduce overfitting this way.\n",
        "\n",
        "To do so, you can use `ImageDataGenerator` from Keras that makes all the work for you (including rescaling), with the following parameter: \n",
        "* `horizontal_flip=True`\n",
        "\n",
        "For more info about how the `ImageDataGenerator` works, you can check out [this article](https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/).\n",
        "\n",
        "Begin by creating an object `ImageDataGenerator` with this parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-19T11:58:37.442182Z",
          "start_time": "2020-08-19T11:58:37.438397Z"
        },
        "id": "pas-fMSIyO8q"
      },
      "outputs": [],
      "source": [
        "# TODO: Instantiate an ImageDataGenerator object\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7nCnu9syO8r"
      },
      "source": [
        "Finally, you can train your model using this generator, with the method `fit_generator` of your model and the method `flow` of your `ImageDataGenerator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt6wXa3IyO8r",
        "outputId": "8c440a78-a84a-421b-9234-21ccd2f035fa",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f5442c0fba65>:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(datagen.flow(X_train_norm, y_train_cat, batch_size=batch_size),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58/58 [==============================] - 4s 56ms/step - loss: 0.5830 - accuracy: 0.8137 - val_loss: 0.4035 - val_accuracy: 0.8565\n",
            "Epoch 2/20\n",
            "58/58 [==============================] - 3s 47ms/step - loss: 0.3966 - accuracy: 0.8580 - val_loss: 0.3760 - val_accuracy: 0.8671\n",
            "Epoch 3/20\n",
            "58/58 [==============================] - 3s 49ms/step - loss: 0.3729 - accuracy: 0.8665 - val_loss: 0.3841 - val_accuracy: 0.8615\n",
            "Epoch 4/20\n",
            "58/58 [==============================] - 3s 54ms/step - loss: 0.3627 - accuracy: 0.8703 - val_loss: 0.3665 - val_accuracy: 0.8683\n",
            "Epoch 5/20\n",
            "58/58 [==============================] - 3s 49ms/step - loss: 0.3561 - accuracy: 0.8708 - val_loss: 0.3615 - val_accuracy: 0.8696\n",
            "Epoch 6/20\n",
            "58/58 [==============================] - 3s 59ms/step - loss: 0.3448 - accuracy: 0.8756 - val_loss: 0.3596 - val_accuracy: 0.8702\n",
            "Epoch 7/20\n",
            "58/58 [==============================] - 3s 48ms/step - loss: 0.3412 - accuracy: 0.8769 - val_loss: 0.3509 - val_accuracy: 0.8747\n",
            "Epoch 8/20\n",
            "58/58 [==============================] - 5s 88ms/step - loss: 0.3314 - accuracy: 0.8798 - val_loss: 0.3539 - val_accuracy: 0.8719\n",
            "Epoch 9/20\n",
            "58/58 [==============================] - 3s 46ms/step - loss: 0.3276 - accuracy: 0.8826 - val_loss: 0.3458 - val_accuracy: 0.8767\n",
            "Epoch 10/20\n",
            "58/58 [==============================] - 3s 48ms/step - loss: 0.3258 - accuracy: 0.8817 - val_loss: 0.3414 - val_accuracy: 0.8760\n",
            "Epoch 11/20\n",
            "58/58 [==============================] - 5s 85ms/step - loss: 0.3214 - accuracy: 0.8834 - val_loss: 0.3474 - val_accuracy: 0.8734\n",
            "Epoch 12/20\n",
            "58/58 [==============================] - 5s 94ms/step - loss: 0.3173 - accuracy: 0.8850 - val_loss: 0.3480 - val_accuracy: 0.8731\n",
            "Epoch 13/20\n",
            "58/58 [==============================] - 3s 57ms/step - loss: 0.3131 - accuracy: 0.8866 - val_loss: 0.3429 - val_accuracy: 0.8739\n",
            "Epoch 14/20\n",
            "58/58 [==============================] - 3s 53ms/step - loss: 0.3127 - accuracy: 0.8866 - val_loss: 0.3418 - val_accuracy: 0.8751\n",
            "Epoch 15/20\n",
            "58/58 [==============================] - 3s 46ms/step - loss: 0.3076 - accuracy: 0.8875 - val_loss: 0.3343 - val_accuracy: 0.8784\n",
            "Epoch 16/20\n",
            "58/58 [==============================] - 3s 58ms/step - loss: 0.3036 - accuracy: 0.8893 - val_loss: 0.3277 - val_accuracy: 0.8801\n",
            "Epoch 17/20\n",
            "58/58 [==============================] - 3s 57ms/step - loss: 0.2997 - accuracy: 0.8907 - val_loss: 0.3310 - val_accuracy: 0.8791\n",
            "Epoch 18/20\n",
            "58/58 [==============================] - 3s 57ms/step - loss: 0.2970 - accuracy: 0.8918 - val_loss: 0.3313 - val_accuracy: 0.8796\n",
            "Epoch 19/20\n",
            "58/58 [==============================] - 3s 48ms/step - loss: 0.2945 - accuracy: 0.8920 - val_loss: 0.3240 - val_accuracy: 0.8823\n",
            "Epoch 20/20\n",
            "58/58 [==============================] - 3s 59ms/step - loss: 0.2938 - accuracy: 0.8919 - val_loss: 0.3294 - val_accuracy: 0.8781\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1544427f10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# TODO: train your model\n",
        "batch_size = 1024\n",
        "model.fit_generator(datagen.flow(X_train_norm, y_train_cat, batch_size=batch_size),\n",
        "                    validation_data=(X_test_norm, y_test_cat), callbacks=callbacks,\n",
        "                    steps_per_epoch=len(X_train_norm) / batch_size, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuzFke8pyO8r"
      },
      "source": [
        "Recompute the accuracy of your model, does it improve your performances with data augmentation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsTm86tuyO8r",
        "outputId": "2f3ffea7-4a5e-49cf-cfd7-a9dd4dd1a6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 0s 3ms/step\n",
            "10/10 [==============================] - 0s 3ms/step\n",
            "accuracy on train with CNN: 0.89615\n",
            "accuracy on test with CNN: 0.8781\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute the accuracy of your model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "batch_size=1024\n",
        "y_pred_train = to_categorical(model.predict(X_train_norm,batch_size=batch_size).argmax(axis=1), num_classes=10)\n",
        "y_pred_test = to_categorical(model.predict(X_test_norm,batch_size=batch_size).argmax(axis=1), num_classes=10)\n",
        "\n",
        "print('accuracy on train with CNN:', accuracy_score(y_pred_train, y_train_cat))\n",
        "print('accuracy on test with CNN:', accuracy_score(y_pred_test, y_test_cat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOzkdGf7yO8s"
      },
      "source": [
        "You can now try to improve even more your results. For example, add more parameters to your `ImageDataGenerator`, play with some hyperparameters, and so on..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
